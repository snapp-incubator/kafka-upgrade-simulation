#Ansible managed

broker.id=1

zookeeper.connect= zookeeper1:2181,zookeeper2:2181,zookeeper3:2181

log.dirs= /var/lib/kafka

# The number of network threads that the server uses for handling network requests. You probably don't need to change this.
num.network.threads=3

# The number of I/O threads that the server uses for executing requests. You should have at least as many threads as you have disks.
num.io.threads=4

# default: 10
# The number of threads to use for various background processing tasks such as file deletion. You should not need to change this.
background.threads=10

# default: null
# Listener List - Comma-separated list of URIs we will listen on and their protocols. Specify hostname as 0.0.0.0 to bind to all interfaces. Leave hostname empty to bind to default interface. Examples of legal listener lists: PLAINTEXT://myhost:9092,TRACE://:9091 PLAINTEXT://0.0.0.0:9092, TRACE://localhost:9093

listeners=SASL://kafka1:9092
listener.security.protocol.map=SASL:SASL_PLAINTEXT
advertised.listeners=SASL://kafka1:9092

inter.broker.listener.name=SASL
sasl.enabled.mechanisms=SCRAM-SHA-512
sasl.mechanism.inter.broker.protocol=SCRAM-SHA-512

authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer
allow.everyone.if.no.acl.found=false
super.users=User:admin

zookeeper.set.acl=false
# default: 100 * 1024
# The SO_SNDBUFF buffer the server prefers for socket connections.
socket.send.buffer.bytes=102400

# default: 100 * 1024
# The SO_RCVBUFF buffer the server prefers for socket connections.
socket.receive.buffer.bytes=102400

# default: 100 * 1024 * 1024
# The maximum request size the server will allow. This prevents the server from running out of memory and should be smaller than the Java heap size.
socket.request.max.bytes=104857600

# The default number of partitions per topic if a partition count isn't given at topic creation time.
num.partitions=1

# default: 1024 * 1024 * 1024
# The log for a topic partition is stored as a directory of segment files. This setting controls the size to which a segment file will grow before a new segment is rolled over in the log. This setting can be overridden on a per-topic basis (see the per-topic configuration section).
log.segment.bytes=1073741824


# default: 7 days
# The amount of time to keep a log segment before it is deleted, i.e. the default data retention window for all topics. Note that if both log.retention.minutes and log.retention.bytes are both set we delete a segment when either limit is exceeded. This setting can be overridden on a per-topic basis (see the per-topic configuration section).
log.retention.hours=168

# default: -1
# The amount of data to retain in the log for each topic-partitions. Note that this is the limit per-partition so multiply by the number of partitions to get the total data retained for the topic. Also note that if both log.retention.hours and log.retention.bytes are both set we delete a segment when either limit is exceeded. This setting can be overridden on a per-topic basis (see the per-topic configuration section).
log.retention.bytes=-1

# default: 5 minutes
# The period with which we check whether any log segment is eligible for deletion to meet the retention policies.
log.retention.check.interval.ms=300000


# default: 1
# The number of threads compacting/deleting expired log segments
log.cleaner.threads=1


# default: Long.MaxValue
# The number of messages written to a log partition before we force an fsync on the log. Setting this lower will sync data to disk more often but will have a major impact on performance. We generally recommend that people make use of replication for durability rather than depending on single-server fsync, however this setting can be used to be extra certain.
log.flush.interval.messages=200000

# default: Long.MaxValue
# The frequency in ms that the log flusher checks whether any log is eligible to be flushed to disk.
log.flush.scheduler.interval.ms=200000


# default: true
# Enable auto creation of topic on the server. If this is set to true then attempts to produce data or fetch metadata for a non-existent topic will automatically create it with the default replication factor and number of partitions.
auto.create.topics.enable=true


# default: 64 * 1024
# The socket receive buffer for network requests to the leader for replicating data.
replica.socket.receive.buffer.bytes=65536


# Number of threads used to replicate messages from leaders. Increasing this value can increase the degree of I/O parallelism in the follower broker.
num.replica.fetchers=5


# default: 6000
# ZooKeeper session timeout. If the server fails to heartbeat to ZooKeeper within this period of time it is considered dead. If you set this too low the server may be falsely considered dead; if you set it too high it may take too long to recognize a truly dead server.
zookeeper.session.timeout.ms=6000

# default: 6000
# The maximum amount of time that the client waits to establish a connection to zookeeper.
zookeeper.connection.timeout.ms=6000


# default: true
# Enable controlled shutdown of the broker. If enabled, the broker will move all leaders on it to some other brokers before shutting itself down. This reduces the unavailability window during shutdown.
controlled.shutdown.enable=true

# Number of retries to complete the controlled shutdown successfully before executing an unclean shutdown.
controlled.shutdown.max.retries=1


# default: 5000
# Backoff time between shutdown retries.
controlled.shutdown.retry.backoff.ms=5000



# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
num.recovery.threads.per.data.dir=4


# default: false
# Enable delete topic.
delete.topic.enable=false


# default: 1440
# Offsets that are older than this age will be marked for deletion. The actual purge will occur when the log cleaner compacts the offsets topic.
offsets.topic.retention.minutes=1440


# The replication factor for the offset commit topic. A higher setting (e.g., three or four) is recommended in order to ensure higher availability. If the offsets topic is created when fewer brokers than the replication factor then the offsets topic will be created with fewer replicas.
offsets.topic.replication.factor=3



min.insync.replicas=3

